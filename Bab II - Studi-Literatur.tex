% ==========================================
% BAB II STUDI LITERATUR
% ==========================================
\chapter{STUDI LITERATUR}

% -------------------------------------------------------------
Bab ini membahas landasan teoritis dan konteks ilmiah yang melatarbelakangi penelitian. Pembahasan diawali dengan tinjauan mengenai \textit{large language models} beserta mekanisme internalnya, termasuk formulasi \textit{autoregressive language modeling}, arsitektur Transformer, serta proses pelatihan dan inferensi yang membentuk sifat probabilistik keluaran model. Selanjutnya dibahas bagaimana persona diperlakukan sebagai konstruksi linguistik dalam interaksi dengan LLM, meliputi definisi, mekanisme kerja, serta perannya sebagai variabel eksperimen. Bagian berikutnya menguraikan benchmark yang digunakan, yaitu GSM8K dan MMLU-Redux, beserta tantangan evaluasi berbasis persona. Bab ini ditutup dengan ringkasan penelitian terdahulu, identifikasi keterbatasan yang masih ada, dan posisi penelitian ini dalam lanskap studi terkait.

\section{Large Language Models}

\textit{Large language models} merupakan fondasi utama dari sistem generatif modern yang digunakan dalam penelitian ini. Perkembangan model berskala besar seperti GPT-3 yang diperkenalkan oleh \textcite{brown2020language} menunjukkan bahwa peningkatan kapasitas model dan jumlah data pelatihan secara signifikan meningkatkan kemampuan representasi serta menghasilkan keluaran yang semakin selaras dengan instruksi dan lebih terstruktur pada berbagai tugas pemrosesan bahasa.

Pemahaman terhadap mekanisme internal \textit{large language models} menjadi penting dalam konteks penelitian ini karena perilaku persona dan variasi penalaran yang diamati merupakan konsekuensi langsung dari sifat probabilistik, struktur arsitektural, dan tujuan pelatihan model. Model bahasa tidak melakukan penalaran simbolik, melainkan mempelajari distribusi token dari data dan menghasilkan keluaran melalui estimasi probabilitas token berikutnya. Dengan demikian, fenomena seperti pergeseran gaya respons, koherensi argumen, atau sensitivitas terhadap persona berakar pada mekanisme internal tersebut.

Selain itu, model berskala besar membawa bias yang terdapat dalam data pelatihan. Analisis oleh \textcite{bender2021stochastic} menunjukkan bahwa data berukuran sangat besar yang tidak terkurasi dapat merepresentasikan ketidakseimbangan sosial, kultur, dan bahasa. Konsekuensinya, \textit{large language models} dapat menginternalisasi dan mereproduksi bias tersebut. Pemahaman mengenai dasar matematis dan arsitektural menjadi penting untuk menjelaskan bagaimana bias tersebut muncul serta bagaimana persona dapat mengubah pola keluaran model.

Subbagian berikut membahas dasar matematis dari \textit{autoregressive language modeling} sebagai komponen fundamental dari sebagian besar \textit{large language models}.

\subsection{Autoregressive Language Modeling}

\textit{Autoregressive language modeling} digunakan oleh sebagian besar \textit{large language models} untuk membentuk distribusi probabilitas atas urutan token melalui prediksi token berikutnya berdasarkan seluruh konteks sebelumnya. Pendekatan ini menyediakan kerangka matematis yang menjelaskan bagaimana keluaran model terbentuk, bagaimana representasi internal berubah ketika konteks dimodifikasi, serta bagaimana instruksi awal seperti persona dapat menghasilkan variasi pola respons.

\subsubsection{Formulasi Probabilistik dan Next-Token Prediction}

Pada \textit{autoregressive language modeling}, probabilitas urutan token $x_1, x_2, \dots, x_T$ difaktorisasi menjadi
\begin{equation}
p(x_1, x_2, \dots, x_T)
=
\prod_{t=1}^{T} p(x_t \mid x_{<t}).
\label{eq:autoregressive-factorization}
\end{equation}

Pendekatan ini diperkenalkan oleh \textcite{bengio2003nlm} dan menjadi fondasi bagi model bahasa berbasis jaringan saraf. Model menghasilkan distribusi token melalui mekanisme \textit{next-token prediction}, di mana setiap prediksi dibentuk dari representasi konteks dalam hidden state. Setiap hidden state merupakan hasil transformasi berulang dari embedding token sebelumnya, sehingga konteks awal seperti instruksi persona secara langsung menentukan bentuk representasi yang mengalir ke langkah-langkah berikutnya.

Distribusi token dihitung melalui fungsi softmax atas nilai logit yang dihasilkan oleh model. Karena fungsi softmax bersifat sensitif terhadap perbedaan kecil pada logit, perubahan kecil pada hidden state akibat instruksi persona dapat menghasilkan pergeseran yang signifikan dalam distribusi probabilitas token berikutnya. Dengan demikian, efek persona muncul sebagai fenomena matematis berupa perubahan representasi konteks yang memodulasi arah prediksi token.

\textcite{brown2020language} menunjukkan bahwa model berskala besar mampu menampilkan pola respons yang mengikuti struktur instruksi pengguna. Dalam \textit{instruction following}, model menghasilkan keluaran yang konsisten dengan pola instruksi dalam data pelatihan. Struktur respons yang mengikuti instruksi tercapai karena model mempelajari hubungan statistik antara bentuk perintah dan rentang respons yang berasosiasi dengannya.

Model juga menghasilkan rangkaian token yang tampak sebagai penjelasan berurutan ketika diberikan tugas tertentu. Pada \textit{contextual reasoning}, urutan token yang dihasilkan membentuk struktur langkah-langkah yang selaras dengan konteks sebelumnya. Struktur ini muncul dari kecocokan probabilistik antartoken dalam embedding space dan tidak bergantung pada mekanisme penalaran eksplisit. Token dipilih berdasarkan kedekatannya secara distribusional terhadap konteks, sehingga rangkaian yang terbentuk tampak menyerupai penalaran.

Efek persona terhadap distribusi token dapat diilustrasikan melalui pergeseran embedding cluster. Instruksi persona dengan gaya formal menghasilkan hidden state yang memberi skor logit lebih tinggi bagi token dengan register formal, sehingga token tersebut menjadi lebih mungkin muncul. Sebaliknya, persona santai menghasilkan distribusi yang memberi preferensi terhadap token informal. Pergeseran ini terjadi pada level representasi, bukan pada perubahan struktur arsitektural.

Selain itu, proses inferensi bersifat autoregresif dan tidak menggunakan token benar seperti pada pelatihan. Ketika model menghasilkan tokennya sendiri, distribusi prediksi dapat mengalami deviasi yang semakin besar seiring panjang urutan, sebuah ketidaksesuaian yang dikenal dengan istilah training–inference mismatch. Kondisi ini memperbesar sensitivitas terhadap konteks awal, sehingga pengaruh persona menjadi lebih menonjol.

\subsubsection{Cross-Entropy Loss dan Implikasi Pelatihan}

Model dilatih dengan mengoptimalkan \textit{cross-entropy loss}, yang mengukur seberapa baik distribusi prediksi model mendekati distribusi token benar dalam data. Objektif ini diformulasikan sebagai
\begin{equation}
\mathcal{L}
=
- \sum_{t=1}^{T} \log p_{\theta}(x_t \mid x_{<t}).
\label{eq:cross-entropy-loss}
\end{equation}

Sebagaimana dijelaskan oleh \textcite{goodfellow2016deep}, optimasi terhadap \textit{cross-entropy loss} mendorong model untuk menyesuaikan parameter sehingga meningkatkan probabilitas token yang benar. Pelatihan ini tidak dirancang untuk mengoptimalkan koherensi semantik atau struktur argumentatif, melainkan untuk meniru distribusi token dalam corpus pelatihan.

Konsekuensi penting dari pendekatan ini adalah terinternalisasinya bias distribusional yang terdapat dalam data pelatihan. \textcite{bender2021stochastic} menunjukkan bahwa corpus berskala besar sering kali memuat ketidakseimbangan representasi linguistik dan sosial. Karena model melakukan estimasi probabilitas berdasarkan pola distribusional tersebut, bias yang tertanam dalam data dapat muncul kembali dalam keluaran model.

Sensitivitas mekanisme autoregresif terhadap konteks awal memperkuat pengaruh persona. Instruksi persona yang muncul pada bagian awal masukan membentuk hidden state awal dan memodulasi jalur prediksi token, sehingga menghasilkan perbedaan konsisten dalam gaya argumentasi, tingkat ketegasan, dan struktur penjelasan meskipun instruksi utamanya sama. Fenomena ini menjadi landasan bagi penelitian ini dalam mengevaluasi bagaimana persona mempengaruhi keluaran model dan persepsi pengguna.

\subsection{Arsitektur Transformer}

Arsitektur Transformer merupakan dasar bagi sebagian besar \textit{large language models} modern. Arsitektur ini dirancang untuk memproses urutan secara efisien melalui mekanisme \textit{self-attention}, yang memungkinkan model membentuk representasi konteks secara global tanpa hambatan ketergantungan sekuensial. Mekanisme ini memiliki peran penting dalam menentukan bagaimana informasi mengalir di dalam model, bagaimana representasi konteks diperbarui di setiap lapisan, serta bagaimana instruksi persona memodulasi distribusi token selama proses prediksi. Secara ringkas, struktur komponen utama Transformer ditunjukkan pada Gambar~\ref{fig:transformer-architecture}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{image/arsitektur-transformer.png}
    \caption{Struktur umum arsitektur Transformer \parencite{vaswani2017attention}}
    \label{fig:transformer-architecture}
\end{figure}

Gambar~\ref{fig:transformer-architecture} menunjukkan aliran informasi melalui mekanisme \textit{attention}, \textit{multi-head integration}, \textit{positional encoding}, \textit{feed-forward block}, \textit{residual connection}, dan \textit{layer normalization} pada setiap lapisan.

\subsubsection{Mekanisme Self-Attention}

Mekanisme \textit{self-attention} menghitung hubungan antartoken melalui representasi query, key, dan value yang diproyeksikan dari token masukan. Formulasi ini dijelaskan oleh \textcite{vaswani2017attention} sebagai
\begin{equation}
\text{Attention}(Q, K, V)
=
\text{softmax}\left(
\frac{QK^{T}}{\sqrt{d_k}}
\right)V,
\label{eq:self-attention}
\end{equation}
di mana $d_k$ adalah dimensi key. Operasi ini memberikan bobot perhatian berdasarkan kecocokan distribusional antara query dan key. Bobot tersebut mengatur kontribusi value dalam pembentukan representasi token, sehingga token dengan relevansi lebih tinggi terhadap konteks akan memiliki pengaruh lebih besar.

Fungsi softmax yang digunakan pada perhatian sensitif terhadap variasi kecil pada nilai logit, sehingga perubahan kecil pada hidden state awal—seperti akibat instruksi persona—dapat menghasilkan perubahan nontrivial pada bobot perhatian. Dengan demikian, persona mempengaruhi jalur informasi sejak lapisan pertama dengan memodifikasi representasi konteks yang digunakan untuk membangun distribusi token selanjutnya.

\subsubsection{Multi-Head Attention dan Layer-Wise Representations}

Komponen \textit{multi-head attention} memperluas mekanisme perhatian dengan memproses beberapa proyeksi query, key, dan value secara paralel. Setiap head mempelajari pola ketergantungan yang berbeda dalam urutan, seperti hubungan sintaktis, asosiasi semantik, koherensi wacana, atau struktur respons yang berulang dalam data pelatihan.

Representasi yang dihasilkan oleh setiap head kemudian digabungkan untuk membentuk layer-wise representations, yaitu representasi token yang diperbarui di setiap lapisan berdasarkan kombinasi informasi dari seluruh head. Lapisan-lapisan Transformer menyusun hierarchical representations yang semakin kaya, karena representasi pada lapisan berikutnya memanfaatkan konteks yang telah diperkaya oleh lapisan sebelumnya.

Dalam konteks persona, modifikasi kecil pada hidden state awal dapat memengaruhi sensitivitas head tertentu terhadap pola bahasa tertentu. Sebagai contoh, persona formal dapat memperkuat kontribusi head yang secara statistik lebih sering terkait dengan struktur kalimat baku, sedangkan persona santai dapat menggeser perhatian ke pola yang lebih percakapan. Efek ini terpropagasi sepanjang lapisan dan berdampak langsung pada distribusi logit yang menentukan token berikutnya.

\subsubsection{Positional Encoding dan Bias Struktural}

Karena \textit{self-attention} tidak mengandung informasi posisi secara inheren, Transformer menggunakan \textit{positional encoding} untuk menggabungkan informasi urutan ke dalam representasi token. Encoding ini memastikan bahwa model dapat membedakan token berdasarkan posisinya, yang penting untuk menjaga struktur urutan bahasa.

Kajian oleh \textcite{liu2024lost} menunjukkan bahwa penggunaan \textit{positional encoding} dan struktur perhatian menyebabkan beberapa bias struktural, termasuk:
\begin{itemize}
    \item \textit{recency bias}, yaitu kecenderungan model memberi bobot perhatian lebih besar pada token yang muncul di akhir konteks,
    \item \textit{positional bias}, yakni sensitivitas yang berbeda terhadap token di posisi tertentu,
    \item penurunan pemanfaatan informasi pada bagian tengah urutan (\textit{lost in the middle}).
\end{itemize}

Bias ini relevan terhadap fenomena persona karena instruksi persona biasanya berada di awal konteks. Representasi awal tersebut tetap berpengaruh kuat terhadap representasi selanjutnya meskipun bagian lain dari urutan berada lebih jauh. Dalam pengaturan eksperimen, kondisi ini membuat efek persona relatif stabil terhadap variasi panjang konteks selama instruksi awal tetap dipertahankan.

Selain itu, hubungan antara positional encoding dan karakteristik persona menjadi penting ketika model menghadapi instruksi di bagian awal konteks. Meskipun recency bias membuat model cenderung memperhatikan token yang lebih baru, representasi awal yang mengandung persona tetap memberi kerangka gaya komunikasi dasar yang tidak mudah terhapus. Fenomena ini menjelaskan mengapa persona tetap konsisten di sepanjang keluaran meskipun konteks teknis atau tugas yang diberikan berubah.




\subsubsection{Feed-Forward Networks, Residual Connection, dan Layer Normalization}

Setiap lapisan Transformer mencakup \textit{feed-forward block} yang menerapkan transformasi nonlinier pada setiap representasi token. Komponen ini memperkaya representasi dengan menambahkan nonlinieritas dan meningkatkan kapasitas model untuk mempelajari pola yang lebih kompleks.

\textit{Residual connection} memungkinkan informasi dari lapisan sebelumnya tetap dipertahankan dan membantu stabilitas propagasi sinyal di sepanjang jaringan. Sementara itu, \textit{layer normalization} menjaga distribusi aktivasi tetap stabil selama pelatihan, sehingga setiap lapisan dapat membentuk representasi token yang konsisten dan dapat diprediksi. Interaksi antara \textit{feed-forward block}, \textit{residual connection}, dan \textit{layer normalization} membentuk representasi hierarkis yang digunakan untuk menghitung skor logit pada setiap langkah prediksi token.

Di samping itu, kombinasi \textit{feed-forward} dan \textit{residual pathway} menciptakan jalur propagasi sinyal yang menjaga stabilitas karakter persona dalam representasi internal. Ketika konteks awal membawa gaya bahasa tertentu, \textit{residual connection} memungkinkan informasi tersebut tetap hadir di setiap lapisan tanpa hilang dalam transformasi nonlinier. \textit{Layer normalization} memastikan bahwa preferensi gaya tersebut tidak hilang sebagai noise, melainkan tetap memengaruhi pembentukan logit pada langkah-langkah berikutnya.

\subsection{Pelatihan dan Inferensi}

Proses pelatihan dan inferensi pada \textit{large language models} memiliki perbedaan mendasar dalam distribusi konteks yang digunakan untuk menghasilkan prediksi. Perbedaan ini menentukan stabilitas keluaran, sensitivitas terhadap instruksi awal, serta konsistensi struktur respons. Memahami dinamika ini penting untuk menjelaskan bagaimana persona dapat memengaruhi pola prediksi model.

\subsubsection{Teacher Forcing dan Exposure Bias}

Selama pelatihan, model menggunakan token benar dari data sebagai konteks pada setiap langkah prediksi melalui prosedur \textit{teacher forcing}. Distribusi yang dipelajari model pada langkah ke-$t$ didasarkan pada probabilitas
\begin{equation}
p_{\theta}(x_t \mid x_{<t}),
\end{equation}
yang dihitung dengan mengondisikan representasi pada token yang benar. Prosedur ini mempercepat konvergensi tetapi menciptakan ketergantungan kuat terhadap distribusi konteks yang tidak muncul pada saat inferensi.

Berbeda dari pelatihan, pada proses inferensi model tidak lagi menerima token benar; model menggunakan token yang dihasilkannya sendiri sebagai konteks berikutnya. Ketidaksesuaian antara kondisi pelatihan dan inferensi ini menimbulkan \textit{exposure bias}, yaitu akumulasi deviasi akibat kesalahan kecil pada tahap awal. Akumulasi ini memperkuat pengaruh konteks awal, termasuk instruksi persona, karena representasi yang terbentuk pada awal urutan digunakan berulang kali pada langkah-langkah selanjutnya.

\subsubsection{Training–Inference Mismatch}

Optimasi selama pelatihan dilakukan dengan meminimalkan \textit{cross-entropy loss}:
\begin{equation}
\mathcal{L}
=
- \sum_{t=1}^{T} \log p_{\theta}(x_t \mid x_{<t}),
\end{equation}
yang mengukur kecocokan model terhadap distribusi token benar. Namun distribusi yang digunakan pada inferensi adalah distribusi yang dibentuk oleh token prediksi model sendiri. Karena token prediksi tersebut dapat menyimpang dari token benar, model bekerja pada konteks yang secara statistik berbeda dari konteks yang digunakan untuk melatihnya. Ketidaksesuaian ini membuat keluaran model sangat sensitif terhadap variasi kecil pada konteks awal, termasuk modifikasi representasi akibat persona.

Perbedaan mendasar antara konteks pelatihan dan inferensi ini menciptakan kondisi di mana setiap penyimpangan pada tahap awal akan terakumulasi sepanjang proses generatif. Ketika representasi awal telah dimodulasi oleh persona, efek tersebut dapat menyebar ke seluruh urutan token karena model terus menggunakan hasil prediksinya sendiri sebagai konteks. Dengan demikian, \textit{training–inference mismatch} memperkuat pengaruh persona dan menjadikannya faktor determinan dalam penentuan gaya, struktur penjelasan, dan konsistensi respons yang dihasilkan model.



\subsubsection{Strategi Decoding dan Dampaknya pada Pola Keluaran}

Inferensi memerlukan pemilihan token dari distribusi probabilitas yang dihitung oleh model. Pemilihan ini ditentukan oleh strategi decoding, yang memainkan peran signifikan dalam membentuk struktur dan koherensi keluaran.

Pendekatan deterministik seperti \textit{greedy decoding} memilih token dengan probabilitas tertinggi pada setiap langkah, menghasilkan respons yang stabil namun kurang variatif. Metode pencarian seperti \textit{beam search} mengevaluasi beberapa kandidat urutan sekaligus sehingga meningkatkan koherensi, meskipun sensitivitas terhadap konteks awal tetap tinggi. Pendekatan berbasis sampling, seperti \textit{top-$k$ sampling} atau \textit{nucleus sampling}, memilih token dari distribusi terpotong dan menghasilkan variasi respons yang lebih besar.

Perbedaan strategi ini memengaruhi struktur keluaran yang tampak seperti reasoning. Respons yang tampak lebih linier dan terkontrol sering muncul pada decoding deterministik, sementara respons yang lebih bervariasi muncul pada pendekatan sampling. Kedua pola tersebut merupakan hasil dinamika probabilistik, bukan hasil dari mekanisme penalaran eksplisit.
\subsubsection{Implikasi Terhadap Pengaruh Persona}

Dinamika pelatihan dan inferensi serta variasi strategi decoding menjelaskan mengapa persona memiliki pengaruh kuat terhadap keluaran model. Instruksi persona ditempatkan pada awal konteks dan langsung membentuk representasi awal pada hidden state. Ketika model memasuki tahap inferensi dan menggunakan keluarannya sendiri sebagai konteks, perbedaan kecil dalam representasi awal akibat persona dapat terakumulasi dan menghasilkan variasi respons yang konsisten. Strategi decoding berbasis sampling memperbesar variasi tersebut, sedangkan pendekatan deterministik memperkuat konsistensi gaya persona tetapi dengan rentang ekspresi yang lebih sempit.

Dengan demikian, pengaruh persona tidak hanya muncul sebagai modifikasi kecil pada awal keluaran, tetapi sebagai pola distribusional yang stabil sepanjang proses generatif. Representasi persona berfungsi sebagai kondisi awal yang mengatur preferensi token di berbagai lapisan dan langkah prediksi, sehingga menghasilkan karakter respons yang konsisten dan dapat diamati secara sistematis dalam eksperimen.

\subsubsection{Implikasi Terhadap Pengaruh Persona}

Dinamika antara proses pelatihan dan inferensi, ditambah variasi strategi decoding, membuat pengaruh persona terhadap keluaran model menjadi cukup menonjol. Instruksi persona yang ditempatkan di awal masukan membentuk kondisi awal representasi internal, sehingga perbedaan kecil pada bagian ini dapat menghasilkan arah generatif yang berbeda untuk urutan token selanjutnya. Temuan mengenai sensitivitas model terhadap cara instruksi dan \textit{prompt} dirumuskan, seperti yang dibahas oleh \textcite{zhou2023largemodelsensitive} dan \textcite{wei2022chain}, mendukung pandangan bahwa pemilihan konteks awal memiliki dampak yang besar terhadap bentuk penalaran dan gaya respons yang muncul.

Ketika model memasuki tahap inferensi dan mulai menggunakan keluarannya sendiri sebagai konteks, representasi awal yang sudah dimodulasi oleh persona akan terus memengaruhi distribusi token pada langkah-langkah berikutnya. Hal ini selaras dengan pengamatan bahwa penjelasan atau \textit{chain-of-thought} yang dihasilkan model tidak selalu mencerminkan proses penalaran internal yang stabil, melainkan sangat bergantung pada cara konteks awal dibingkai \parencite{turpin2023language}. Dalam kerangka tersebut, persona dapat dipahami sebagai salah satu bentuk \textit{framing} yang secara sistematis mengarahkan gaya bahasa, struktur penjelasan, dan tingkat ketegasan respons model \parencite{gupta2024biasrunsdeep,tseng2024twotales}.

Dengan demikian, persona tidak sekadar menambahkan nuansa gaya pada kalimat pembuka, tetapi berfungsi sebagai kondisi awal yang membentuk preferensi token sepanjang proses generatif. Efek ini membuat karakteristik persona muncul secara konsisten di berbagai bagian respons, sehingga perbedaan antarpersona dapat diamati dan dibandingkan secara sistematis dalam pengaturan eksperimen yang terkontrol.

\subsection{Sumber Bias dalam LLM}

Bias dalam \textit{large language models} muncul sebagai konsekuensi dari berbagai faktor yang terlibat dalam proses pembentukan model, mulai dari karakteristik data pelatihan, struktur arsitektur, tujuan optimasi, hingga prosedur penyelarasan dengan preferensi manusia. Sejumlah tinjauan dan studi evaluasi komprehensif menunjukkan bahwa model berskala besar membawa peluang sekaligus risiko yang berkaitan dengan representasi sosial, keadilan, dan potensi dampak negatif \parencite{weidinger2021ethical,bommasani2021opportunities,liang2023helm}. Bias tidak hadir sebagai keputusan eksplisit, melainkan sebagai hasil dari pembelajaran pola distribusional yang tidak selalu seimbang.

Memahami sumber-sumber bias ini penting untuk menjelaskan bagaimana pola tertentu muncul dalam keluaran model dan bagaimana instruksi persona dapat memperkuat atau memodulasi kecenderungan tersebut. Dalam konteks penelitian ini, persona diperlakukan sebagai variabel yang berinteraksi dengan bias yang sudah tertanam dalam model, sehingga dapat membantu mengungkap bagaimana kecenderungan tertentu muncul pada tugas penalaran dan skenario evaluasi yang berbeda \parencite{gupta2024biasrunsdeep,tseng2024twotales}.

\subsubsection{Bias Berbasis Data}

Corpus pelatihan LLM umumnya terdiri dari data berukuran sangat besar yang dikumpulkan dari berbagai sumber daring. \textcite{bender2021stochastic} menekankan bahwa data semacam ini hampir tidak mungkin bebas dari ketidakseimbangan representasi sosial, linguistik, maupun kultural. Kajian lain menunjukkan bahwa model berskala besar cenderung merefleksikan pola dan perspektif yang dominan dalam data pelatihan, sehingga risiko reproduksi bias menjadi sulit dihindari \parencite{weidinger2021ethical,bommasani2021opportunities}.

Model yang dioptimalkan untuk menyesuaikan diri terhadap distribusi tersebut akan cenderung menghasilkan token atau pola yang memang sering muncul dalam corpus. Ketika persona diperkenalkan, instruksi tersebut berinteraksi dengan bias data yang sudah ada. Persona formal, misalnya, dapat memperkuat kecenderungan model untuk memilih struktur kalimat baku dan terminologi teknis, sedangkan persona kasual lebih mudah memicu gaya percakapan yang banyak ditemukan pada teks daring. Interaksi ini menunjukkan bahwa persona bekerja di dalam lanskap bias yang sudah tertanam, bukan sebagai faktor yang sepenuhnya terpisah dari distribusi data pelatihan \parencite{gupta2024biasrunsdeep}.

\subsubsection{Bias Struktural}

Selain data, arsitektur Transformer juga berkontribusi terhadap munculnya bias tertentu. \textcite{vaswani2017attention} menunjukkan bahwa mekanisme \textit{self-attention} dan \textit{positional encoding} memungkinkan model memanfaatkan informasi konteks secara global, namun studi lanjutan menemukan adanya pola perhatian yang tidak merata. \textcite{liu2024lost} mengidentifikasi \textit{recency bias}, yaitu kecenderungan model memberikan bobot perhatian lebih besar pada token di bagian akhir konteks, serta fenomena \textit{lost in the middle}, di mana informasi pada bagian tengah urutan panjang kurang termanfaatkan secara optimal.

Bias struktural ini dapat berinteraksi dengan instruksi persona yang ditempatkan di awal konteks. Meskipun secara posisional persona berada di awal, representasi yang terbentuk pada lapisan-lapisan awal tetap dipertahankan melalui mekanisme seperti \textit{residual connection} dan pembentukan representasi hierarkis \parencite{jurafsky2023slp3}. Akibatnya, karakter persona cenderung bertahan sepanjang proses generatif meskipun konteks lanjutan memanjang atau berubah. Dalam pengaturan ini, persona berfungsi sebagai jangkar yang menstabilkan arah generatif model, sementara bias struktural menentukan bagaimana informasi lain di urutan diprioritaskan.

\subsubsection{Bias Objektif Pelatihan}

Tujuan optimasi model yang berfokus pada minimisasi \textit{cross-entropy loss} membentuk kecenderungan model untuk memprediksi token yang konsisten dengan pola dalam data pelatihan. Objektif ini tidak dirancang untuk menilai kualitas argumen, ketepatan struktur penjelasan, ataupun koherensi wacana, sehingga model akan lebih banyak mengikuti pola yang muncul secara dominan dalam corpus. Kondisi ini membuat model sensitif terhadap distribusi data dan bentuk instruksi yang diberikan pada konteks awal.

Selain itu, perbedaan antara kondisi pelatihan dan inferensi—sering disebut sebagai \textit{training–inference mismatch}—dapat menyebabkan penyimpangan arah generatif ketika model harus menggunakan prediksinya sendiri sebagai konteks berikutnya. Fenomena ini didokumentasikan dalam berbagai studi mengenai pembelajaran sekuensial \parencite{ranzato2016sequence}. Penelitian terkait kalibrasi dan teknik \textit{prompting} menunjukkan bahwa keluaran model sangat dipengaruhi oleh kondisi distribusional yang dihadapi saat inferensi \parencite{zhao2021calibration,liu2023pretrain}. Dalam konteks persona, hal ini berarti bahwa instruksi pada awal konteks dapat mengarahkan struktur keluaran secara konsisten karena sensitivitas model terhadap kondisi awal tersebut.

\subsubsection{Bias Alignment}

Tahap penyelarasan model dengan preferensi manusia, seperti melalui \textit{reinforcement learning from human feedback} (RLHF), turut membentuk karakter respons model. Proses ini membuat model belajar menghasilkan bentuk respons tertentu yang dianggap lebih sesuai dengan instruksi atau contoh yang diberikan selama pelatihan \parencite{ouyang2022training}. Karena proses penyelarasan tersebut menggunakan contoh-contoh khusus, model dapat mengembangkan pola respons yang tidak sepenuhnya simetris terhadap seluruh variasi instruksi.

Dalam konteks persona, interaksi dengan pola respons hasil penyelarasan dapat terlihat dalam variasi gaya penjelasan atau struktur keluaran. Persona yang dirumuskan dengan gaya yang mirip dengan contoh pada tahap penyelarasan cenderung menghasilkan keluaran yang lebih seragam, sedangkan persona dengan gaya berbeda dapat mendorong model mengikuti pola token yang tidak terlalu sering muncul dalam contoh penyelarasan. Hal ini menunjukkan bahwa persona berperan sebagai faktor yang memodulasi arah keluaran model, bukan sebagai sumber bias baru.

%==== 2.2.1

\section{Persona sebagai Konstruksi Linguistik dalam Interaksi LLM}

Persona dalam konteks \textit{large language models} tidak merujuk pada sifat psikologis atau karakter manusia, tetapi pada instruksi linguistik yang ditempatkan di awal konteks untuk mengarahkan model menghasilkan respons dengan gaya, register, atau struktur tertentu. Instruksi tersebut berfungsi sebagai sinyal kondisional yang memodulasi representasi awal dalam hidden state, sehingga memengaruhi jalur prediksi token selama proses autoregresif.

Penelitian mengenai \textit{prompt-based conditioning} menunjukkan bahwa perubahan kecil pada formulasi konteks dapat menghasilkan keluaran yang berbeda secara signifikan \parencite{schick2021pet, zhao2021calibration}. Model bahasa merespons pola instruksi berdasarkan distribusi representasi yang telah dipelajari selama pra-pelatihan dan penyelarasan. Oleh karena itu, pemahaman tentang persona penting untuk memastikan bahwa penggunaan persona sebagai variabel eksperimen memiliki landasan teoretis yang jelas.

Subbagian berikut membahas definisi persona dalam model bahasa serta mekanisme teknis yang membuat persona mampu memodulasi keluaran model.

\subsection{Definisi Persona dalam Konteks Model Bahasa}

Persona dalam model bahasa dipahami sebagai instruksi linguistik yang membingkai cara model menghasilkan respons. Instruksi ini dapat berupa pernyataan peran (\textit{role prompt}), deskripsi gaya komunikasi, atau konteks mengenai karakteristik pengguna yang ditempatkan pada awal masukan. Instruksi tersebut memengaruhi representasi awal yang terbentuk melalui embedding token dan hidden state pertama, sehingga mengubah distribusi probabilitas token pada langkah-langkah berikutnya selama generasi.

Literatur mengenai \textit{prompt-based learning} menunjukkan bahwa model sangat sensitif terhadap struktur dan formulasi instruksi yang diberikan \parencite{schick2021pet}. Pola respons yang selaras dengan instruksi bukan merupakan bentuk penalaran laten, tetapi hasil dari kecocokan distribusional antara instruksi dan representasi yang dipelajari selama pra-pelatihan. Dengan kata lain, persona berperan sebagai sinyal yang menggeser distribusi prediktif model tanpa membangun struktur kepribadian atau preferensi yang stabil.

Penelitian mengenai kalibrasi konteks menunjukkan bahwa bahkan variasi kecil dalam deskripsi atau framing dapat menghasilkan perbedaan yang berarti dalam keluaran model \parencite{zhao2021calibration}. Instruksi seperti “You are a formal academic assistant” dan “Your user is a university student” bekerja melalui mekanisme yang sama: keduanya memodifikasi representasi awal, yang kemudian menentukan pola prediksi token sepanjang proses autoregresif. Dengan demikian, persona dipandang sebagai alat linguistik yang mempengaruhi keluaran melalui perubahan kondisi awal, bukan sebagai entitas dengan perilaku internal.

\subsection{Mekanisme Persona dalam Model Autoregresif}

Efek persona dalam model autoregresif muncul dari cara model membentuk representasi konteks pada langkah awal inferensi. Instruksi persona dimasukkan sebagai token pertama dan diproses melalui embedding layer serta lapisan awal Transformer. Proses ini menghasilkan hidden state awal yang digunakan untuk menghitung distribusi probabilitas token pertama, dan hidden state tersebut menjadi dasar bagi seluruh langkah prediksi berikutnya. Perubahan kecil pada representasi awal dapat menghasilkan perbedaan signifikan karena sifat propagatif dari mekanisme autoregresif.

Mekanisme \textit{self-attention} memperkuat efek ini. Setiap token dalam instruksi persona berkontribusi pada perhitungan perhatian melalui operasi $\text{softmax}\!\left(\frac{QK^{T}}{\sqrt{d_k}}\right)$, sehingga memodulasi bobot yang menentukan bagaimana representasi konteks dibangun pada setiap lapisan. Perubahan distribusi perhatian tersebut menyebabkan pergeseran representasi yang memengaruhi logit pada seluruh langkah generasi. Hasilnya, persona tidak hanya mengubah gaya bahasa, tetapi juga memengaruhi struktur respons dan pola penjelasan yang dihasilkan model.

Konsistensi efek persona diperkuat oleh sifat autoregresif model: token yang dihasilkan pada langkah awal menjadi bagian dari konteks untuk langkah berikutnya. Fenomena ini sejalan dengan \textit{training–inference mismatch} dan \textit{exposure bias}, di mana deviasi kecil pada konteks awal diperkuat sepanjang urutan \parencite{liu2023pretrain}. Akibatnya, persona dapat menghasilkan pergeseran sistematis dalam keluaran meskipun instruksi tugas tetap sama.

Temuan empiris mengenai teknik prompting, seperti \textit{chain-of-thought prompting} \parencite{wei2022chain}, menunjukkan bahwa modifikasi konteks awal berdampak langsung pada struktur penjelasan dan pola reasoning yang ditampilkan model. Hal ini mendukung pemahaman bahwa persona adalah sinyal kondisional yang bekerja melalui mekanisme representasi awal dan propagasi token dalam model autoregresif. Oleh karena itu, analisis mekanisme ini menjadi dasar penting bagi penggunaan persona dalam penelitian ini.

\subsection{Klasifikasi Persona dalam Literatur}

Penelitian mengenai persona pada \textit{large language models} menunjukkan bahwa pemberian identitas atau gaya pengguna pada konteks awal dapat menggeser pola penalaran, struktur respons, serta tingkat kehati-hatian model \parencite{gupta2024biasrunsdeep}. Survei komprehensif mengenai persona dan personalisasi pada LLM membedakan antara persona di sisi pengguna, persona yang menetapkan peran pada model, dan skema personalisasi jangka panjang \parencite{tseng2024twotales}. Berdasarkan batasan metodologis penelitian ini, hanya persona di sisi pengguna yang digunakan, yaitu persona eksplisit, persona implisit, dan persona netral sebagai baseline.

\subsubsection{Persona eksplisit}

Persona eksplisit menyatakan identitas pengguna secara langsung melalui deskripsi identitas pada system message. Identitas dirumuskan berdasarkan beberapa dimensi yang relevan seperti gender, rentang usia, agama, pekerjaan, kewarganegaraan, dan register bahasa \parencite{gupta2024biasrunsdeep}. Dalam penelitian ini, persona eksplisit direalisasikan melalui deskripsi identitas yang ditempatkan pada awal konteks, tanpa memberikan informasi tambahan terkait jawaban atau strategi penyelesaian soal. Format teknis seperti “Your user is …” digunakan sebagai implementasi praktis dari \textit{identity descriptor} yang dijelaskan pada penelitian persona-assigned models, meskipun tidak berasal dari satu paper tertentu. Deskripsi identitas ini bekerja sebagai sinyal kondisional yang memodulasi representasi awal sehingga mempengaruhi struktur penjelasan atau langkah-langkah reasoning yang dipilih model.

\subsubsection{Persona implisit melalui gaya tutur}

Persona implisit tidak menyebutkan identitas pengguna secara eksplisit, tetapi dibentuk melalui narasi pengalaman, pilihan diksi, atau gaya tutur pada masukan pengguna. Survei \textcite{tseng2024twotales} menunjukkan bahwa LLM dapat menginferensi persona dari isyarat linguistik semacam ini, sehingga gaya tutur dapat berfungsi sebagai bentuk persona tersirat. Penelitian mengenai sensitivitas model terhadap framing prompt menemukan bahwa variasi kecil dalam formulasi bahasa dapat menghasilkan perbedaan yang sistematis dalam gaya respons atau tingkat perincian penjelasan \parencite{zhou2023largemodelsensitive, zhao2021calibrate}. Dalam penelitian ini, persona implisit diberikan melalui paragraf naratif yang merepresentasikan sudut pandang pengguna. Representasi tersirat ini mendorong model untuk menyesuaikan register dan pola penalaran berdasarkan interpretasinya terhadap karakter pengguna yang muncul dari gaya bahasanya.

\subsubsection{Persona netral}

Persona netral digunakan sebagai baseline ketika tidak ada sinyal identitas atau gaya tambahan yang diberikan. Pada kondisi ini, system message hanya berfokus pada instruksi tugas tanpa menyebutkan gender, usia, pekerjaan, atau atribut sosial lain. Baseline diperlukan untuk memisahkan efek persona eksplisit dan implisit dari variasi yang mungkin muncul akibat struktur instruksi atau noise dalam proses decoding. Studi mengenai ketidaksetiaan penjelasan model pada reasoning \parencite{turpin2023language} menekankan pentingnya baseline yang jelas ketika mengevaluasi pergeseran pola reasoning, sehingga persona netral menjadi komponen metodologis penting dalam penelitian ini.

Ruang lingkup penelitian ini tidak mencakup role-playing persona yang menetapkan identitas tertentu pada model, maupun pendekatan personalisasi jangka panjang yang melibatkan penyimpanan profil pengguna. Survei \textcite{tseng2024twotales} serta kajian risiko etis pada model bahasa \parencite{weidinger2021ethical, bommasani2021opportunities} menunjukkan bahwa personalisasi jangka panjang dan role-playing persona membawa implikasi metodologis serta risiko bias yang berbeda dari persona berbasis konteks linguistik yang digunakan dalam penelitian ini.

\subsection{Persona sebagai Variabel Eksperimental dalam Penelitian Ini}

Dalam penelitian ini, persona diperlakukan sebagai variabel eksperimen yang memodulasi kondisi awal pada proses generasi token tanpa mengubah isi soal, struktur tugas, atau informasi kunci yang dibutuhkan untuk menjawab pertanyaan. Persona hanya memengaruhi framing identitas pengguna dan gaya komunikasi, sehingga setiap variasi keluaran dapat dikaitkan secara langsung dengan perbedaan konteks linguistik.

\subsubsection{Konfigurasi persona dan dimensi identitas}

Persona disusun berdasarkan enam dimensi identitas yang muncul dalam penelitian sebelumnya, yaitu gender, rentang usia, agama, pekerjaan, kewarganegaraan, dan register bahasa \parencite{gupta2024biasrunsdeep}. Dimensi tersebut digunakan untuk membentuk himpunan persona eksplisit dan implisit yang konsisten, terstruktur, dan dapat direplikasi. Pada persona eksplisit, seluruh dimensi dituliskan secara langsung dalam system message sebagai deskripsi identitas. Pada persona implisit, dimensi tersebut direpresentasikan secara tersirat melalui narasi pengguna sehingga model perlu menginferensikannya dari gaya tutur \parencite{tseng2024twotales}.

\subsubsection{Integrasi persona dalam pipeline eksperimen}

Penerapan persona dilakukan melalui dua tahap, yaitu persona context initialization dan persona warm-up message. Tahap pertama membentuk konteks identitas melalui system message. Tahap kedua berupa satu interaksi pemanasan yang digunakan untuk memastikan bahwa model mengikuti gaya persona sebelum mengerjakan soal. Pendekatan kalibrasi konteks ini sejalan dengan temuan bahwa performa few-shot LLM sangat sensitif terhadap formulasi instruksi dan framing awal \parencite{zhao2021calibrate, liu2023pretrain}.

Setelah kalibrasi, seluruh soal dalam benchmark dijalankan dalam kondisi persona yang sama. Pelaksanaan kombinasi persona–model–benchmark diatur melalui pendekatan \textit{spec-driven experiment orchestration}, yaitu eksperimen yang disusun terlebih dahulu di dalam berkas spesifikasi formal sebelum dijalankan secara otomatis oleh \textit{pipeline}. Penelitian ini menggunakan GSM8K untuk menguji penalaran aritmetika berbasis soal cerita \parencite{cobbe2021gsm8k}, serta MMLU-Redux 2.0 untuk mengevaluasi kemampuan penalaran multi-bidang \parencite{hendryckstest2021, gema2024arewedonewithmmlu, mmluRedux2024dataset}.

\subsubsection{Kontrol struktur prompt dan pengaruh framing}

Struktur prompt dibuat seragam pada seluruh model dan seluruh persona agar variabel yang berubah hanyalah konteks identitas dan gaya tutur. Instruksi tugas tidak diubah dan berada dalam format yang sama, sedangkan bagian yang bervariasi hanya system message untuk persona eksplisit dan gaya tutur pada masukan pengguna untuk persona implisit. Penelitian mengenai sensitivitas model terhadap framing \parencite{zhou2023largemodelsensitive} menegaskan bahwa desain prompt harus dikontrol ketat untuk memastikan bahwa perbedaan keluaran memang berasal dari persona, bukan variasi teknis lain.

Dengan desain ini, persona berfungsi sebagai faktor kondisional yang memengaruhi representasi awal pada hidden state, sesuai dengan mekanisme autoregresif yang dijelaskan pada Subbab~2.1. Variasi keluaran pada benchmark dapat dianalisis sebagai konsekuensi langsung dari perubahan konteks linguistik di awal interaksi, bukan dari modifikasi parameter model atau struktur tugas.

\subsection{Efek Persona terhadap Keluaran Model}

Persona berfungsi sebagai sinyal kondisional yang membentuk representasi awal pada hidden state, sehingga memodulasi jalur prediksi token selama proses autoregresif. Efek persona muncul bukan karena model memiliki pemahaman tentang identitas pengguna, tetapi karena model menafsirkan deskripsi identitas atau gaya tutur sebagai bagian dari konteks linguistik yang mempengaruhi pembobotan perhatian, pemilihan token, dan struktur penjelasan. Penelitian mengenai reasoning bias pada persona-assigned models menunjukkan bahwa perubahan kecil dalam deskripsi identitas dapat menyebabkan pergeseran sistematis dalam pola penalaran \parencite{gupta2024biasrunsdeep}. Selain itu, sensitivitas LLM terhadap framing instruksi \parencite{zhou2023largemodelsensitive} dan pentingnya kalibrasi konteks awal \parencite{zhao2021calibrate} mendukung bahwa persona berpotensi mempengaruhi keluaran meskipun tugas yang diberikan tetap sama.

Efek persona dalam penelitian ini dianalisis melalui tiga mekanisme utama, yaitu pergeseran register dan gaya respons, perubahan struktur penjelasan, dan modifikasi jalur reasoning.

\subsubsection{Pergeseran register dan gaya respons}

Persona eksplisit dan implisit dapat mengubah register bahasa, tingkat formalitas, atau preferensi gaya penyampaian model. Variasi ini terjadi karena deskripsi identitas atau gaya tutur mempengaruhi distribusi representasi pada lapisan awal Transformer. Framing linguistik yang berbeda telah terbukti menghasilkan respons yang berbeda meskipun instruksi tugas sama \parencite{zhou2023largemodelsensitive}. Dengan demikian, persona dapat menyebabkan keluaran yang lebih formal, lebih ringkas, atau lebih naratif, meskipun jawaban yang benar tidak berubah. Pergeseran gaya ini penting untuk dianalisis agar tidak disalahartikan sebagai variasi kemampuan model.

\subsubsection{Perubahan struktur penjelasan}

Persona juga dapat memodulasi kecenderungan model untuk memberikan penjelasan panjang, ringkas, berhati-hati, atau langsung ke jawaban. Perubahan struktur penjelasan sejalan dengan temuan bahwa model dapat memberikan penjelasan yang terdengar rasional tetapi tidak selalu merefleksikan proses reasoning internal \parencite{turpin2023language}. Karena itu, persona yang mendorong gaya tertentu—seperti persona akademik atau persona yang berbicara santai—dapat mempengaruhi format penjelasan model tanpa mempengaruhi validitas langkah reasoning yang sebenarnya diperlukan untuk menyelesaikan soal.

\subsubsection{Modifikasi jalur reasoning}

Efek paling penting dari persona adalah pergeseran pada langkah-langkah reasoning yang dipilih model. Penelitian \textcite{gupta2024biasrunsdeep} menunjukkan bahwa deskripsi identitas dapat mengubah preferensi model terhadap pola reasoning tertentu, seperti memilih penjelasan yang lebih hati-hati, lebih sistematis, atau lebih cepat menuju jawaban. Dalam konteks penelitian ini, tugas penalaran pada GSM8K dan MMLU-Redux sangat sensitif terhadap perubahan konteks awal karena model mengakumulasi informasi secara autoregresif. Persona implisit melalui narasi gaya tutur juga dapat mendorong model menafsirkan situasi sosial atau emosi tertentu sebelum memulai reasoning, sebagaimana ditunjukkan oleh temuan mengenai inferensi persona tersirat \parencite{tseng2024twotales}. Hal ini dapat menggeser struktur reasoning meskipun konten soal identik.

\subsubsection{Implikasi terhadap evaluasi model}

Efek persona harus diperlakukan sebagai variabel eksperimental yang mempengaruhi keluaran model, bukan sebagai indikator perubahan kemampuan. Evaluasi holistik \parencite{liang2023helm} menekankan bahwa model harus diuji pada berbagai kondisi untuk memahami sensitivitasnya terhadap variasi konteks. Dengan demikian, analisis persona dalam penelitian ini tidak hanya mengevaluasi apakah model memperoleh jawaban yang benar, tetapi juga bagaimana perubahan framing identitas dan gaya tutur mempengaruhi keandalan reasoning, kestabilan respons, dan konsistensi performa lintas model.

Dengan kerangka ini, persona dipahami sebagai faktor linguistik yang menggeser dinamika prediksi token, sehingga mengubah jalur reasoning dan struktur respons tanpa mengubah akses model terhadap informasi atau kemampuan umum yang dimilikinya.

%======= 2.3
\section{Evaluasi Benchmark}

Benchmark digunakan sebagai instrumen evaluasi yang memberikan ukuran terstandarisasi terhadap kemampuan penalaran \textit{large language models}. Melalui benchmark, performa model dapat dibandingkan secara konsisten lintas persona, model, dan skenario instruksi. Evaluasi berbasis benchmark juga penting dalam konteks penelitian ini karena keluaran reasoning LLM dapat dipengaruhi oleh framing linguistik, termasuk variasi persona yang diberikan di awal konteks \parencite{zhou2023largemodelsensitive}. 

Selain itu, penelitian menunjukkan bahwa penjelasan yang dihasilkan model tidak selalu mencerminkan proses penalaran internal, tetapi dapat berupa penjelasan yang tidak setia (\textit{unfaithful}) terhadap mekanisme prediksi yang sebenarnya digunakan \parencite{turpin2023language}. Oleh karena itu, benchmark diperlukan untuk menyediakan dasar evaluasi yang objektif ketika menganalisis bagaimana persona memengaruhi struktur reasoning dan keputusan model.

Dalam penelitian ini digunakan dua benchmark yang saling melengkapi, yaitu GSM8K dan MMLU-Redux. Keduanya mewakili dua bentuk penalaran yang berbeda: penalaran numerik prosedural dan penalaran konseptual berbasis pengetahuan.

\subsection{GSM8K}

GSM8K merupakan benchmark untuk mengevaluasi \textit{numerical reasoning} melalui soal cerita matematika tingkat sekolah dasar \parencite{cobbe2021gsm8k}. Setiap soal membutuhkan identifikasi informasi penting, penyusunan langkah-langkah perhitungan yang logis, serta penarikan kesimpulan secara runtut. Meskipun sederhana bagi manusia, struktur reasoning ini menantang bagi LLM karena model harus menghasilkan urutan token yang menyerupai alur penyelesaian multi-langkah.

GSM8K relevan dalam konteks persona karena penalaran numerik yang bersifat prosedural terbukti sensitif terhadap variasi framing instruksi. Penelitian mengenai sensitivitas LLM terhadap perubahan gaya prompt menunjukkan bahwa perbedaan kecil dalam formulasi konteks dapat menggeser struktur langkah reasoning yang dihasilkan \parencite{zhou2023largemodelsensitive}. Hal ini memungkinkan persona memengaruhi panjang penjelasan, tingkat kehati-hatian, atau bentuk argumentasi yang ditampilkan model selama menyelesaikan soal numerik.

\subsection{MMLU-Redux}

MMLU-Redux adalah versi kurasi ulang dari benchmark MMLU yang mengevaluasi kemampuan penalaran konseptual dan pemahaman lintas disiplin \parencite{mmluRedux2024dataset}. Benchmark ini mencakup berbagai bidang seperti sains, humaniora, hukum, kedokteran, dan ilmu sosial. Berbeda dari GSM8K, tugas dalam MMLU-Redux disajikan dalam format \textit{multiple-choice}, sehingga model harus memilih jawaban yang paling tepat berdasarkan representasi pengetahuan dan pemahaman konsep.

Karena format evaluasi bersifat tertutup, MMLU-Redux memudahkan pengamatan terhadap pergeseran preferensi jawaban yang muncul akibat variasi persona. Sensitivitas terhadap framing telah dibahas dalam literatur \parencite{zhou2023largemodelsensitive}, sehingga perubahan gaya linguistik pada konteks awal dapat memengaruhi kecenderungan model dalam memilih opsi tertentu meskipun informasi faktual tidak berubah.

\subsection{Tantangan Evaluasi Berbasis Persona}

Evaluasi berbasis persona menghadapi beberapa tantangan metodologis. Tantangan pertama adalah memastikan bahwa perubahan respons disebabkan oleh persona, bukan karena variasi formulasi instruksi. Karena LLM sangat peka terhadap struktur prompt dan pilihan kata \parencite{zhao2021calibrate}, penelitian ini menggunakan format prompt yang sepenuhnya konsisten untuk seluruh eksperimen.

Tantangan kedua adalah variabilitas keluaran model. LLM dapat memberikan respons berbeda meskipun instruksi identik, terutama pada tugas yang melibatkan reasoning multi-langkah \parencite{turpin2023language}. Untuk mengurangi variabilitas tersebut, proses evaluasi diotomatisasi dan seluruh benchmark dijalankan dalam konfigurasi deterministik yang seragam.

Dengan demikian, GSM8K dan MMLU-Redux memberikan dua perspektif berbeda tentang bagaimana persona memengaruhi reasoning. GSM8K memperlihatkan efek persona pada struktur reasoning prosedural, sedangkan MMLU-Redux menunjukkan bagaimana framing identitas pengguna dapat menggeser preferensi jawaban dalam tugas konseptual. Kombinasi keduanya memberikan fondasi metodologis yang kuat untuk analisis pada Bab IV dan Bab V.

\section{Penelitian Terdahulu dan Kesenjangan Penelitian}

Penelitian mengenai persona pada \textit{large language models} menunjukkan bahwa variasi identitas pengguna, gaya tutur, dan framing instruksi dapat memengaruhi pola penalaran dan bentuk respons model. Kajian ini relevan dengan mekanisme internal LLM yang dijelaskan pada Subbab~2.1 dan sensitivitas model terhadap konteks awal yang dijelaskan pada Subbab~2.2. Meskipun demikian, literatur yang ada masih menyisakan sejumlah pertanyaan mendasar mengenai sejauh mana persona memengaruhi reasoning dalam struktur evaluasi yang terukur dan terstandarisasi.

\subsection{Ringkasan Literatur Terkait}

Gupta \parencite{gupta2024biasrunsdeep} menunjukkan bahwa persona eksplisit dapat menggeser langkah penalaran yang dihasilkan model, bahkan ketika isi tugas tetap sama. Temuan ini mengindikasikan bahwa identitas yang ditempatkan pada konteks awal tidak hanya memengaruhi gaya bahasa, tetapi juga struktur reasoning.

Tseng \parencite{tseng2024twotales} menegaskan bahwa persona tidak hanya muncul melalui deklarasi identitas, tetapi juga melalui pola bahasa yang implisit. Model dapat menafsirkan ciri pengguna dari pilihan kata dan narasi, kemudian menyesuaikan respons sesuai interpretasi tersebut. Kondisi ini konsisten dengan mekanisme pembentukan representasi awal yang dijelaskan pada Subbab~2.2.

Turpin \parencite{turpin2023language} menunjukkan bahwa reasoning yang dihasilkan LLM sering kali tidak stabil dan dapat berubah akibat variasi kecil dalam prompt. Penelitian ini memperkuat pemahaman bahwa penalaran model bukan proses simbolik, melainkan hasil dinamika distribusi token yang sangat sensitif terhadap konteks.

Selain itu, penelitian mengenai risiko bias menunjukkan bahwa LLM dapat memunculkan pola sosial yang tidak seimbang sebagai konsekuensi dari data pelatihan \parencite{weidinger2021ethical, bommasani2021opportunities}. Ketika persona tertentu diperkenalkan, bias yang sudah ada dapat teramplifikasi atau termodulasi.

Penelitian terkait sensitivitas prompt \parencite{zhou2023largemodelsensitive} dan kalibrasi konteks \parencite{zhao2021calibrate} juga menunjukkan bahwa framing linguistik di awal interaksi dapat mengubah respons model secara signifikan. Hasil ini memperkuat argumen bahwa persona, sebagai bentuk framing, dapat memengaruhi reasoning yang dihasilkan model.

\subsection{Keterbatasan Penelitian Sebelumnya}

Meskipun kontribusi penelitian terdahulu penting, beberapa keterbatasan masih terlihat jelas.

Pertama, sebagian besar penelitian persona hanya menguji sedikit model dan tidak melakukan analisis lintas-LLM. Hal ini menyebabkan sulitnya menggeneralisasi bagaimana pengaruh persona berbeda antarmodel.

Kedua, variasi persona yang digunakan pada studi sebelumnya sering kali terbatas pada beberapa contoh representatif, sehingga belum menangkap spektrum identitas pengguna yang lebih luas. Sebaliknya, penelitian ini menggunakan himpunan persona eksplisit dan implisit yang lebih beragam.

Ketiga, sedikit penelitian yang mengevaluasi persona dalam konteks benchmark reasoning yang terstandarisasi. Banyak studi berfokus pada dialog atau tugas generatif yang tidak memiliki jawaban benar salah, sehingga efek persona sulit diukur secara objektif.

Keempat, tidak semua penelitian memastikan konsistensi struktur prompt. Karena LLM sangat sensitif terhadap perubahan formulasi instruksi \parencite{zhou2023largemodelsensitive, zhao2021calibrate}, perbedaan kecil dalam prompt berpotensi mencemari hasil analisis persona.

Kelima, stabilitas reasoning jarang dievaluasi pada benchmark yang berbeda secara kognitif, misalnya penalaran numerik (GSM8K) versus penalaran konseptual (MMLU-Redux). Padahal, persona dapat berdampak berbeda pada tiap jenis tugas.

Keenam, kerangka evaluasi LLM yang umum digunakan—seperti HELM \parencite{liang2023helm}, LM Evaluation Harness, dan OpenAI Evals—belum dirancang untuk mengevaluasi pengaruh persona sebagai variabel eksperimen. Framework-framework tersebut berfokus pada pengujian model terhadap benchmark terstandarisasi dengan prompt yang statis, sehingga tidak mendukung integrasi persona eksplisit maupun implisit, \textit{warm-up} konteks, ataupun variasi framing identitas pengguna. Selain itu, kerangka tersebut tidak menyediakan mekanisme untuk mengeksekusi kombinasi \textit{multi model}~$\times$~\textit{multi persona}~$\times$~\textit{multi benchmark} secara otomatis serta tidak menyimpan keluaran lengkap yang diperlukan untuk menganalisis perubahan struktur penjelasan, gaya bahasa, atau pola \textit{bias} berbasis persona. Akibatnya, pendekatan evaluasi yang ada belum mampu menangani kebutuhan metodologis penelitian ini secara menyeluruh.

\subsection{Posisi dan Kontribusi Penelitian Ini}

Penelitian ini dirancang untuk mengisi kesenjangan tersebut melalui beberapa kontribusi utama.

Pertama, penelitian ini mengevaluasi beberapa model LLM secara paralel, sehingga memungkinkan analisis komparatif mengenai perbedaan sensitivitas persona antarmodel.

Kedua, penelitian ini menggunakan himpunan persona eksplisit dan implisit yang dirancang secara sistematis dan selaras dengan kerangka teoretis pada Subbab~2.2, sehingga variasi pengaruh persona dapat diamati secara lebih komprehensif.

Ketiga, penelitian ini menggunakan dua benchmark reasoning yang memiliki karakteristik kognitif berbeda—GSM8K untuk penalaran numerik prosedural dan MMLU-Redux untuk penalaran konseptual berbasis pilihan ganda. Pendekatan ini menyediakan analisis yang lebih kaya mengenai bagaimana persona memengaruhi bentuk reasoning yang berbeda.

Keempat, seluruh evaluasi dijalankan dalam \textit{pipeline} terotomatisasi dengan struktur prompt yang benar-benar seragam melalui pendekatan \textit{spec-driven experiment orchestration}, mengikuti rekomendasi penelitian mengenai sensitivitas prompt \parencite{zhou2023largemodelsensitive}. Hal ini memastikan bahwa variasi keluaran dapat ditelusuri secara jelas ke persona, bukan ke perbedaan instruksi.

Dengan demikian, penelitian ini tidak hanya mereplikasi studi tentang persona, tetapi memperluas ruang analisis melalui evaluasi lintas-model, lintas-persona, dan lintas-benchmark. Formulasi ini memberikan kontribusi empiris baru mengenai bagaimana identitas pengguna memengaruhi pola reasoning dalam \textit{large language models}.
